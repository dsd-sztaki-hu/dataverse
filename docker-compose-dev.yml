version: "2.4"

services:

  dev_dataverse:
    container_name: "dev_dataverse"
    hostname: dataverse
    image: ${APP_IMAGE}
    restart: on-failure
    user: payara
    environment:
      DATAVERSE_DB_HOST: postgres
      DATAVERSE_DB_PASSWORD: secret
      DATAVERSE_DB_USER: ${DATAVERSE_DB_USER}
      ENABLE_JDWP: "1"
      ENABLE_RELOAD: "1"
      SKIP_DEPLOY: "${SKIP_DEPLOY}"
      DATAVERSE_JSF_REFRESH_PERIOD: "1"
      DATAVERSE_FEATURE_API_BEARER_AUTH: "1"
      DATAVERSE_MAIL_SYSTEM_EMAIL: "dataverse@localhost"
      DATAVERSE_MAIL_MTA_HOST: "smtp"
      DATAVERSE_AUTH_OIDC_ENABLED: "1"
      DATAVERSE_AUTH_OIDC_CLIENT_ID: test
      DATAVERSE_AUTH_OIDC_CLIENT_SECRET: 94XHrfNRwXsjqTqApRrwWmhDLDHpIYV8
      DATAVERSE_AUTH_OIDC_AUTH_SERVER_URL: http://keycloak.mydomain.com:8090/realms/test
      DATAVERSE_SPI_EXPORTERS_DIRECTORY: "/dv/exporters"
      # These two oai settings are here to get HarvestingServerIT to pass
      dataverse_oai_server_maxidentifiers: "2"
      dataverse_oai_server_maxrecords: "2"
      JVM_ARGS: -Ddataverse.files.storage-driver-id=file1
        -Ddataverse.files.file1.type=file
        -Ddataverse.files.file1.label=Filesystem
        -Ddataverse.files.file1.directory=${STORAGE_DIR}/store
        -Ddataverse.files.localstack1.type=s3
        -Ddataverse.files.localstack1.label=LocalStack
        -Ddataverse.files.localstack1.custom-endpoint-url=http://localstack:4566
        -Ddataverse.files.localstack1.custom-endpoint-region=us-east-2
        -Ddataverse.files.localstack1.bucket-name=mybucket
        -Ddataverse.files.localstack1.path-style-access=true
        -Ddataverse.files.localstack1.upload-redirect=true
        -Ddataverse.files.localstack1.download-redirect=true
        -Ddataverse.files.localstack1.access-key=default
        -Ddataverse.files.localstack1.secret-key=default
        -Ddataverse.files.minio1.type=s3
        -Ddataverse.files.minio1.label=MinIO
        -Ddataverse.files.minio1.custom-endpoint-url=http://minio:9000
        -Ddataverse.files.minio1.custom-endpoint-region=us-east-1
        -Ddataverse.files.minio1.bucket-name=mybucket
        -Ddataverse.files.minio1.path-style-access=true
        -Ddataverse.files.minio1.upload-redirect=false
        -Ddataverse.files.minio1.download-redirect=false
        -Ddataverse.files.minio1.access-key=4cc355_k3y
        -Ddataverse.files.minio1.secret-key=s3cr3t_4cc355_k3y
        -Ddataverse.pid.providers=fake
        -Ddataverse.pid.default-provider=fake
        -Ddataverse.pid.fake.type=FAKE
        -Ddataverse.pid.fake.label=FakeDOIProvider
        -Ddataverse.pid.fake.authority=10.5072
        -Ddataverse.pid.fake.shoulder=FK2/
        -Ddataverse.files.sztaki.type=s3
        -Ddataverse.files.sztaki.label=SZTAKI
        -Ddataverse.files.sztaki.custom-endpoint-url=http://minio:9000
        -Ddataverse.files.sztaki.custom-endpoint-region=us-east-1
        -Ddataverse.files.sztaki.bucket-name=sztakibucket
        -Ddataverse.files.sztaki.path-style-access=true
        -Ddataverse.files.sztaki.upload-redirect=false
        -Ddataverse.files.sztaki.download-redirect=false
        -Ddataverse.files.sztaki.access-key=4cc355_k3y
        -Ddataverse.files.sztaki.secret-key=s3cr3t_4cc355_k3y
        -Ddataverse.files.sztaki.directory=/dv/store/sztaki
        -Ddataverse.files.wigner.type=s3
        -Ddataverse.files.wigner.label=Wigner
        -Ddataverse.files.wigner.custom-endpoint-url=http://minio:9000
        -Ddataverse.files.wigner.custom-endpoint-region=us-east-1
        -Ddataverse.files.wigner.bucket-name=wignerbucket
        -Ddataverse.files.wigner.path-style-access=true
        -Ddataverse.files.wigner.upload-redirect=false
        -Ddataverse.files.wigner.download-redirect=false
        -Ddataverse.files.wigner.access-key=4cc355_k3y
        -Ddataverse.files.wigner.secret-key=s3cr3t_4cc355_k3y
        -Ddataverse.files.wigner.directory=/dv/store/wigner
    ports:
      - "8080:8080" # HTTP (Dataverse Application)
      # - "4949:4848" # HTTPS (Payara Admin Console)
      # - "9009:9009" # JDWP
      - "4848:4848" # HTTP (Payara Admin Console)
      - "19009:9009" # JDWP, ARP override
      - "8686:8686" # JMX
    networks:
      - dataverse
    depends_on:
      - dev_postgres
      - dev_solr
      - dev_dv_initializer
    volumes:
      - ./docker-dev-volumes/app/data:/dv
      - ./docker-dev-volumes/app/secrets:/secrets
      - ./target/dataverse:/opt/payara/deployments/dataverse:ro
      # ARP specific
      # Map the glassfish applications folder so that we can update webapp resources using scripts/cpwebapp.sh
      - ./docker-dev-volumes/glassfish/applications:/opt/payara/appserver/glassfish/domains/domain1/applications
      # Uncomment for changes to xhtml to be deployed immediately (if supported your IDE or toolchain).
      # Replace 6.0 with the current version.
      # - ./target/dataverse-6.0:/opt/payara/deployments/dataverse
    tmpfs:
      - /dumps:mode=770,size=2052M,uid=1000,gid=1000
      - /tmp:mode=770,size=2052M,uid=1000,gid=1000
    mem_limit: 2147483648 # 2 GiB
    mem_reservation: 1024m
    privileged: false
    # ARP specific
    # Note: env vr substitution doesn't work with fabric8 (ie. mvn -Pct docker:run), so we need to use
    # explicit arp.orgx base domain here. Also, need host-gateway instead of host.docker.internal
    extra_hosts:
      - "artifact.arp.orgx:host-gateway"
      - "auth.arp.orgx:host-gateway"
      - "cedar.arp.orgx:host-gateway"
      - "component.arp.orgx:host-gateway"
      - "group.arp.orgx:host-gateway"
      - "impex.arp.orgx:host-gateway"
      - "internals.arp.orgx:host-gateway"
      - "messaging.arp.orgx:host-gateway"
      - "open.arp.orgx:host-gateway"
      - "openview.arp.orgx:host-gateway"
      - "repo.arp.orgx:host-gateway"
      - "resource.arp.orgx:host-gateway"
      - "schema.arp.orgx:host-gateway"
      - "submission.arp.orgx:host-gateway"
      - "terminology.arp.orgx:host-gateway"
      - "user.arp.orgx:host-gateway"
      - "valuerecommender.arp.orgx:host-gateway"
      - "worker.arp.orgx:host-gateway"
      - "demo.cee.arp.orgx:host-gateway"
      - "demo-dist.cee.arp.orgx:host-gateway"
      - "docs.cee.arp.orgx:host-gateway"
      - "docs-dist.cee.arp.orgx:host-gateway"
      - "api-php.cee.arp.orgx:host-gateway"
      - "artifacts.arp.orgx:host-gateway"
      - "monitor.arp.orgx:host-gateway"
      - "monitoring.arp.orgx:host-gateway"
      - "bridging.arp.orgx:host-gateway"
      - "arp-dataverse.arp.orgx:host-gateway"
#      - "artifact.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "auth.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "cedar.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "component.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "group.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "impex.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "internals.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "messaging.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "open.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "openview.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "repo.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "resource.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "schema.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "submission.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "terminology.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "user.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "valuerecommender.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "worker.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "demo.cee.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "demo-dist.cee.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "docs.cee.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "docs-dist.cee.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "api-php.cee.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "artifacts.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "monitor.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "monitoring.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "bridging.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"
#      - "arp-dataverse.${CEDAR_BASE_DOMAIN:-arp.orgx}:host.docker.internal"

  dev_bootstrap:
    container_name: "dev_bootstrap"
    image: gdcc/configbaker:unstable
    restart: "no"
    command:
      - bootstrap.sh
      - dev
    networks:
      - dataverse

  dev_dv_initializer:
    container_name: "dev_dv_initializer"
    image: gdcc/configbaker:unstable
    restart: "no"
    command:
      - sh
      - -c
      - "fix-fs-perms.sh dv"
    volumes:
      - ./docker-dev-volumes/app/data:/dv

  dev_postgres:
    container_name: "dev_postgres"
    hostname: postgres
    image: postgres:${POSTGRES_VERSION}
    restart: on-failure
    environment:
      - POSTGRES_USER=${DATAVERSE_DB_USER}
      - POSTGRES_PASSWORD=secret
    ports:
      - "5432:5432"
    networks:
      - dataverse
    volumes:
      - ./docker-dev-volumes/postgresql/data:/var/lib/postgresql/data

  dev_solr_initializer:
    container_name: "dev_solr_initializer"
    image: gdcc/configbaker:unstable
    restart: "no"
    command:
      - sh
      - -c
      - "fix-fs-perms.sh solr && cp -a /template/* /solr-template"
    volumes:
      - ./docker-dev-volumes/solr/data:/var/solr
      - ./docker-dev-volumes/solr/conf:/solr-template

  dev_solr:
    container_name: "dev_solr"
    hostname: "solr"
    image: solr:${SOLR_VERSION}
    depends_on:
      - dev_solr_initializer
    restart: on-failure
    ports:
      - "8983:8983"
    networks:
      - dataverse
    command:
      - "solr-precreate"
      - "collection1"
      - "/template"
    volumes:
      - ./docker-dev-volumes/solr/data:/var/solr
      - ./docker-dev-volumes/solr/conf:/template

  dev_smtp:
    container_name: "dev_smtp"
    hostname: "smtp"
    image: maildev/maildev:2.0.5
    restart: on-failure
    ports:
      - "25:25" # smtp server
      - "1080:1080" # web ui
    environment:
      - MAILDEV_SMTP_PORT=25
      - MAILDEV_MAIL_DIRECTORY=/mail
    networks:
      - dataverse
    #volumes:
    #  - ./docker-dev-volumes/smtp/data:/mail
    tmpfs:
      - /mail:mode=770,size=128M,uid=1000,gid=1000

  dev_keycloak:
    container_name: "dev_keycloak"
    image: 'quay.io/keycloak/keycloak:21.0'
    hostname: keycloak
    environment:
      - KEYCLOAK_ADMIN=kcadmin
      - KEYCLOAK_ADMIN_PASSWORD=kcpassword
      - KEYCLOAK_LOGLEVEL=DEBUG
      - KC_HOSTNAME_STRICT=false
    networks:
      dataverse:
        aliases:
          - keycloak.mydomain.com #create a DNS alias within the network (add the same alias to your /etc/hosts to get a working OIDC flow)
    command: start-dev --import-realm --http-port=8090  # change port to 8090, so within the network and external the same port is used
    ports:
      - "8090:8090"
    volumes:
      - './conf/keycloak/test-realm.json:/opt/keycloak/data/import/test-realm.json'

  # This proxy configuration is only intended to be used for development purposes!
  # DO NOT USE IN PRODUCTION! HIGH SECURITY RISK!
  dev_proxy:
    image: caddy:2-alpine
    # The command below is enough to enable using the admin gui, but it will not rewrite location headers to HTTP.
    # To achieve rewriting from https:// to http://, we need a simple configuration file
    #command: ["caddy", "reverse-proxy", "-f", ":4848", "-t", "https://dataverse:4848", "--insecure"]
    command: ["caddy", "run", "-c", "/Caddyfile"]
    ports:
      - "4848:4848" # Will expose Payara Admin Console (HTTPS) as HTTP
    restart: always
    volumes:
      - ./conf/proxy/Caddyfile:/Caddyfile:ro
    depends_on:
      - dev_dataverse
    networks:
      - dataverse

  dev_localstack:
    container_name: "dev_localstack"
    hostname: "localstack"
    image: localstack/localstack:2.3.2
    restart: on-failure
    ports:
      - "127.0.0.1:4566:4566"
    environment:
      - DEBUG=${DEBUG-}
      - DOCKER_HOST=unix:///var/run/docker.sock
      - HOSTNAME_EXTERNAL=localstack
    networks:
      - dataverse
    volumes:
      - ./conf/localstack:/etc/localstack/init/ready.d
    tmpfs:
      - /localstack:mode=770,size=128M,uid=1000,gid=1000

  dev_minio:
    container_name: "dev_minio"
    hostname: "minio"
    image: minio/minio
    restart: on-failure
    ports:
      - "19000:9000" # ARP override
      - "19001:9001" # ARP override
    networks:
      - dataverse
    volumes:
      - ./docker-dev-volumes/minio_storage:/data
    environment:
      MINIO_ROOT_USER: 4cc355_k3y
      MINIO_ROOT_PASSWORD: s3cr3t_4cc355_k3y
    command: server /data

  createbuckets:
    image: minio/mc
    depends_on:
      - dev_minio
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc config host add myminio http://minio:9000 4cc355_k3y s3cr3t_4cc355_k3y;
      /usr/bin/mc mb myminio/sztakibucket;
      /usr/bin/mc policy set readwrite myminio/sztakibucket;
      /usr/bin/mc mb myminio/wignerbucket;
      /usr/bin/mc policy set readwrite myminio/wignerbucket;
      exit 0;
      "
    networks:
      - dataverse
    volumes:
      - ./docker-dev-volumes/minio:/data

#  s3fs-sztaki:
#    image: efrecon/s3fs:1.80
#    devices:
#      - /dev/fuse
#    cap_add:
#      - SYS_ADMIN
#    security_opt:
#      - apparmor=unconfined
#    environment:
#      - S3FS_DEBUG=1
#      - AWS_S3_BUCKET=sztakibucket
#      - AWS_S3_ACCESS_KEY_ID=4cc355_k3y
#      - AWS_S3_SECRET_ACCESS_KEY=s3cr3t_4cc355_k3y
#      - S3FS_ARGS="-o allow_other -o use_path_request_style -o url=http://minioX:9000"
#      - UID=${UID:-1000}
#      - GID=${GID:-1000}
#    volumes:
#      - ./docker-dev-volumes/app/data/store/sztaki:/opt/s3fs/bucket:rshared
#    networks:
#      - dataverse
#    depends_on:
#      - dev_minio
#
#
#  s3fs-wigner:
#    image: efrecon/s3fs:1.80
#    devices:
#      - /dev/fuse
#    cap_add:
#      - SYS_ADMIN
#    security_opt:
#      - apparmor=unconfined
#    environment:
#      - AWS_S3_BUCKET=wignerbucket
#      - AWS_S3_ACCESS_KEY_ID=4cc355_k3y
#      - AWS_S3_SECRET_ACCESS_KEY=s3cr3t_4cc355_k3y
#      - UID=${UID:-1000}
#      - GID=${GID:-1000}
#    volumes:
#      - ./docker-dev-volumes/app/data/store/wigner:/opt/s3fs/bucket:rshared



  #
  # ARP specific containers
  #

  solr-updater:
    hostname: "solr-updater"
    image: dataverse-solr-updater:latest
    container_name: "dv-solr-updater"
    environment:
      #- DATAVERSE_URL=http://host.docker.internal:8080
      - DATAVERSE_URL=http://dataverse:8080
      - SOLR_URL=http://solr:8983
      - SOLR_SCHEMA_XML_PATH=/var/solr/data/collection1/conf/schema.xml
    ports:
      - '8984:3000'
    volumes:
      - ./docker-dev-volumes/solr/data/data:/var/solr/data
    networks:
      - dataverse

  dataverse-rocrate-preview:
    hostname: "rocrate-preview"
    image: dataverse-rocrate-preview:latest
    ports:
      - '8985:8082'
    networks:
      - dataverse

networks:
  dataverse:
    driver: bridge
